# ML-UEBA – Machine Learning Module

This document explains **only the ML part** of the project:  
how the models are organized, how they work, and how to train / use them.

The ML module takes **featured logs + embeddings** and produces:

- Per-event anomaly scores  
- Per-event model scores (Isolation Forest, LSTM-AE)  
- Concept drift flags (Page-Hinkley)  

These outputs can then be used for **UEBA** and SOC alerting.

---

## 1. ML Folder Structure

```text
ml/
├── README.md                # <-- this file
├── config.py                # ML hyperparameters & paths
├── data_interface.py        # Builds ML feature matrix from df + embeddings
│
├── models/
│   ├── __init__.py
│   ├── base.py              # BaseAnomalyModel interface
│   ├── isolation_forest.py  # IsolationForestModel (unsupervised)
│   ├── lstm_autoencoder.py  # LSTMAutoencoder (unsupervised, deep)
│   └── page_hinkley.py      # PageHinkley drift detector
│
├── utils/
│   ├── __init__.py
│   ├── io.py                # ensure_dir, small IO helpers
│   └── scaling.py           # StandardScaler fit/save/load/transform
│
├── pipeline/
│   ├── __init__.py
│   └── trainer.py           # trains scaler + IF + LSTM-AE
│
└── ml_pipeline.py           # main inference pipeline (used from test.py)
````

---

## 2. Models Used & Why

### 2.1 IsolationForestModel (`models/isolation_forest.py`)

* **Type**: Unsupervised anomaly detection
* **Library**: scikit-learn
* **Use**: Detects **rare / unusual events** in high-dimensional feature space
* **Why**:

  * Works without labels
  * Robust on large log datasets
  * Standard baseline for anomaly detection in security/log analytics

The pipeline uses **Isolation Forest** to get a per-event anomaly score:
`iforest_score`.

---

### 2.2 LSTMAutoencoder (`models/lstm_autoencoder.py`)

* **Type**: Unsupervised sequence autoencoder
* **Library**: PyTorch
* **Use**: Learns **normal behavior** and flags deviations via reconstruction error
* **Why**:

  * Captures non-linear, temporal/structural patterns
  * Suitable for UEBA-style behavior modeling
  * Complements Isolation Forest

The pipeline uses **LSTM-AE** to get another anomaly-like score:
`lstm_score` (reconstruction error).

---

### 2.3 Ensemble Anomaly Score

At inference time, the pipeline:

1. Normalizes `iforest_score` and `lstm_score` to [0, 1]
2. Computes a combined anomaly score:

```text
anomaly_score = 0.5 * iforest_norm + 0.5 * lstm_norm
```

This combined score is exposed as `anomaly_score` per event.

---

### 2.4 PageHinkley (`models/page_hinkley.py`)

* **Type**: Online drift detection over anomaly scores
* **Use**: Detects **behavioral or concept drift** in the stream of anomaly scores
* **Why Page-Hinkley**:

  * Pure Python (no compiled extensions)
  * Works in streaming mode
  * Detects gradual/sudden mean shifts in scores

Output column:
`concept_drift` (boolean per event)

---

## 3. Data Inputs to ML

The ML pipeline assumes that **preprocessing + NLP + feature engineering** are already done.

It expects:

1. A processed CSV (with features)

   * Path: `data/processed/sample.csv`
2. Message embeddings (as a NumPy array)

   * Path: `data/processed/sample_msg_emb.npy`

These are generated by `test.py`:

* `df` from:

  * `PreprocessPipeline` (logs → normalized df)
  * `NLPPipeline` (entities + embeddings)
  * `FeaturePipeline` (behavioral features)
* `embeddings` from:

  * `NLPPipeline` → `message_embedder` (MiniLM model)

The embeddings are saved via:

```python
np.save("data/processed/sample_msg_emb.npy", embeddings)
```

---

## 4. Training the ML Models

Training is **unsupervised** (no labels required).

### 4.1 Prerequisites

Before training, you MUST run:

```bash
uv run test.py
```

This will generate:

* `data/processed/sample.csv`
* `data/processed/sample_msg_emb.npy`

---

### 4.2 Train Command

From the project root (`ML-UEBA/`), run:

```bash
uv run -m ml.pipeline.trainer
```

Or, if not using `uv`:

```bash
python -m ml.pipeline.trainer
```

### 4.3 What `trainer.py` Does

1. Loads:

   * `data/processed/sample.csv`
   * `data/processed/sample_msg_emb.npy`
2. Builds feature matrix `X` using `ml/data_interface.py`
3. Fits `StandardScaler` and saves it
4. Trains:

   * `IsolationForestModel` on `X_scaled`
   * `LSTMAutoencoder` on `X_scaled`
5. Saves artifacts into `models/`:

```text
models/
├── scaler.joblib
├── iforest.joblib
└── lstm_ae.joblib
```

These files are later loaded by `ml_pipeline.py` for inference.

---

## 5. Inference (Using the ML Pipeline)

The main inference entrypoint is the `MLPipeline` class:

```python
from ml.ml_pipeline import MLPipeline

ml = MLPipeline()
df_with_ml = ml.run(df_features, embeddings)
```

### 5.1 What `MLPipeline.run()` Does

Given:

* `df_features` : DataFrame with engineered features
* `embeddings`  : NumPy array with message embeddings (N × D)

It will:

1. Build feature matrix `X`:

   * Using `ml/data_interface.build_feature_matrix`
2. Apply the saved scaler
3. Compute:

   * `iforest_score` via `IsolationForestModel`
   * `lstm_score` via `LSTMAutoencoder`
4. Normalize scores and compute `anomaly_score`
5. Run `PageHinkley.update()` for each `anomaly_score`

   * Produces `concept_drift` flags
6. Return a new DataFrame with extra columns:

```text
iforest_score
lstm_score
anomaly_score
concept_drift
```

These are appended to the existing `df`.

---

## 6. Typical End-to-End Flow (Where ML Fits)

In `test.py` (simplified):

```python
# 1) PREPROCESS
df = preprocess.run(logs)

# 2) NLP + embeddings
df, embeddings = nlp.run(df)

# 3) Feature engineering
df = features.run(df)

# 4) ML inference (after training is done)
from ml.ml_pipeline import MLPipeline
ml = MLPipeline()
df = ml.run(df, embeddings)
```

Now `df` contains:

* Original normalized fields (timestamp, user, src_ip, host, message, etc.)
* Feature-engineered columns
* ML outputs:

  * `iforest_score`
  * `lstm_score`
  * `anomaly_score`
  * `concept_drift`

This can be pushed to:

* DB
* Elasticsearch
* Kafka
* Dashboard / SOC tools

---

## 7. When to Retrain

You should consider retraining when:

* Concept drift is often `True` (Page-Hinkley fires frequently)
* New data distribution is significantly different
* Major infra changes (new subnets, hosts, services, etc.)

Retraining is as simple as:

```bash
uv run test.py          # regenerate csv + embeddings (optional)
uv run -m ml.pipeline.trainer
```

---

## 8. Key Design Choices (Short Justification)

* **Unsupervised models** (Isolation Forest, LSTM-AE)
  → No labels required, robust for security logs.

* **Ensemble of two models**
  → Combines structural anomalies (IF) and behavioral deviations (LSTM-AE).

* **Page-Hinkley drift detection**
  → Lightweight, pure Python, ideal for anomaly score streams.

* **Separated training vs inference**
  → Train once, reuse many times; production-friendly.

---

```
