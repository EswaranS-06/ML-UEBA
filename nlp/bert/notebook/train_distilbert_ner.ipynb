{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ DistilBERT NER Training for Log Intelligence\n",
    "This notebook trains a lightweight **DistilBERT log-specific NER model** that identifies:\n",
    "\n",
    "- USER\n",
    "- SRC_IP\n",
    "- DEST_IP\n",
    "- PROCESS\n",
    "- HOST\n",
    "- PORT\n",
    "- ACTION\n",
    "\n",
    "The trained model will be used as a **fallback** inside your NLP pipeline when regex and rule-based extraction fail.\n",
    "\n",
    "Model is saved to:\n",
    "\n",
    "```\n",
    "nlp/bert/model/\n",
    "  config.json\n",
    "  pytorch_model.bin\n",
    "  tokenizer.json\n",
    "  vocab.txt\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eswar\\Desktop\\ML-UEBA\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     AutoTokenizer,\n\u001b[0;32m      6\u001b[0m     AutoModelForTokenClassification,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     DataCollatorForTokenClassification\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Training Data\n",
    "Your raw labeled data should be located here:\n",
    "\n",
    "```\n",
    "nlp/data/raw/train_logs.txt\n",
    "nlp/data/raw/test_logs.txt\n",
    "```\n",
    "\n",
    "Each line should contain a log and manually annotated BIO tags OR generated using a preprocessing script.\n",
    "\n",
    "Format example:\n",
    "\n",
    "```\n",
    "Invalid O\n",
    "user O\n",
    "admin B-USER\n",
    "from O\n",
    "122.225.109.208 B-SRC_IP\n",
    "port O\n",
    "443 B-PORT\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to raw training data\n",
    "train_path = \"../data/raw/train_logs.txt\"\n",
    "test_path = \"../data/raw/test_logs.txt\"\n",
    "\n",
    "def load_bio_file(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if words:\n",
    "                    sentences.append(words)\n",
    "                    labels.append(tags)\n",
    "                words = []\n",
    "                tags = []\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                w, t = line.split()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            words.append(w)\n",
    "            tags.append(t)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = load_bio_file(train_path)\n",
    "test_sentences, test_labels = load_bio_file(test_path)\n",
    "\n",
    "print(\"Loaded training samples:\", len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_path = \"../bert/utils/label_map.json\"\n",
    "with open(label_map_path, \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "id2label = {int(k): v for k, v in label_map.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "label_list = list(label2id.keys())\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(\"Labels:\", label_list)\n",
    "print(\"# labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_and_align(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    new_labels = []\n",
    "    for i, label_seq in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "\n",
    "        prev_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned.append(-100)\n",
    "            else:\n",
    "                aligned.append(label2id[label_seq[word_id]])\n",
    "\n",
    "        new_labels.append(aligned)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Prepare HuggingFace Datasets\n",
    "train_dataset = Dataset.from_dict(tokenize_and_align(train_sentences, train_labels))\n",
    "test_dataset = Dataset.from_dict(tokenize_and_align(test_sentences, test_labels))\n",
    "\n",
    "data = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../data/output/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../model/\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"Model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/output/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
