# Embeddings Module

The `nlp/embeddings/` package provides **local semantic embeddings** for log messages.  
It uses the **MiniLM (all-MiniLM-L6-v2)** model to generate high-quality vector representations of log text.

These embeddings allow downstream ML models (Isolation Forest, Autoencoder, clustering, UEBA scoring) to detect:

‚Ä¢ unusual login attempts  
‚Ä¢ strange error messages  
‚Ä¢ unseen behavioral patterns  
‚Ä¢ new attack signatures  
‚Ä¢ rare combinations of actions  

All embeddings are:

‚Ä¢ **Fully offline**  
‚Ä¢ **Fast** (CPU-friendly)  
‚Ä¢ **High-quality**  
‚Ä¢ **384-dimensional semantic vectors**  

---

# üìÇ Folder Structure

```
nlp/embeddings/
    README.md
    message_embedder.py
    local_model_setup.py
    model/
        all-MiniLM-L6-v2/        (downloaded locally)
```

---

# üß† Why Embeddings?

Traditional string matching cannot capture:

‚Ä¢ semantic patterns  
‚Ä¢ meaning  
‚Ä¢ similarity between log messages  
‚Ä¢ phrase-level variations  
‚Ä¢ unseen attack patterns  

MiniLM embeddings allow the system to numerically represent logs as:

```python
vector shape: (N_logs √ó 384)
```

These vectors enable:

‚Ä¢ anomaly detection  
‚Ä¢ clustering  
‚Ä¢ correlation of similar events  
‚Ä¢ UEBA behavior scoring  

---

# üß© message_embedder.py

This file loads the MiniLM model and converts log messages into embeddings.

Example:

```python
from nlp.embeddings.message_embedder import MessageEmbedder

embedder = MessageEmbedder()
emb = embedder.encode_message("Failed password for admin from 1.2.3.4")

print(emb.shape)
# (384,)
```

Batch embedding:

```python
emb_matrix = embedder.encode_dataframe(df, "message")
```

---

# ‚öôÔ∏è local_model_setup.py

This script downloads MiniLM model locally into:

```
nlp/embeddings/model/all-MiniLM-L6-v2/
```

Run once:

```bash
uv run python nlp/embeddings/local_model_setup.py
```

This ensures:

‚Ä¢ no internet required  
‚Ä¢ offline and deterministic NLP  
‚Ä¢ faster loading times  

---

# üì¶ Stored Model

After running setup, you get:

```
nlp/embeddings/model/all-MiniLM-L6-v2/
config.json
pytorch_model.bin
tokenizer.json
modules.json
vocab files
```

The `MessageEmbedder` auto-loads from this path.

---

# üöÄ Usage in Pipeline

Embeddings are integrated inside the main NLP pipeline:

```python
df_enriched, emb_matrix = nlp.run(df_preprocessed)
```

Output:

‚Ä¢ df_enriched ‚Äî enriched DataFrame  
‚Ä¢ emb_matrix ‚Äî array of shape (num_logs, 384)  

Used by:

‚Ä¢ Feature Engineering  
‚Ä¢ ML Anomaly Models  
‚Ä¢ UEBA risk scoring  
‚Ä¢ Similarity search engines  

---

# üß™ Example

Message:

```
Invalid user admin from 187.12.249.74
```

Embedding output:

```python
[ -0.14,  0.22, -0.03, ... ]  # length 384
```

Similar logs have similar embeddings.

---

# üõ† How to Replace the Model

Modify the model path:

```python
MessageEmbedder(model_path="nlp/embeddings/model/my_model")
```

Supported models:

‚Ä¢ MiniLM  
‚Ä¢ MPNet  
‚Ä¢ DistilBERT  
‚Ä¢ Custom fine-tuned models  

---

# üü¢ Summary

The Embeddings Module provides:

‚úî Local MiniLM embedding model  
‚úî CPU-friendly vectorization  
‚úî Batch encoding for log messages  
‚úî Drop-in integration with NLP pipeline  
‚úî Semantic representation of logs  
‚úî Strong foundation for UEBA ML models  

This is a core component for understanding log behavior beyond simple string parsing.

